{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextRANK",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "08peKbn9b7po"
      },
      "source": [
        "import json\n",
        "import re\n",
        "!pip install konlpy\n",
        "import konlpy\n",
        "\n",
        "test = []\n",
        "for line in open('extractive_test_v2.jsonl', 'r'):\n",
        "    test.append(json.loads(line))\n",
        "\n",
        "test_article=[]\n",
        "\n",
        "for i in test:\n",
        "    test_article.append(i['article_original'])\n",
        "\n",
        "for i in test_article:\n",
        "    for j,item in enumerate(i):\n",
        "        i[j]=re.compile('[^ ㄱ-ㅣ가-힣]+').sub('',item)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFQ3qIjOdgLI"
      },
      "source": [
        "splited_test=test_article\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "for item in splited_test:\n",
        "    for i,j in enumerate(item):\n",
        "        item[i]=okt.morphs(j)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8oryLtijd7U"
      },
      "source": [
        "stop_words=[]\n",
        "\n",
        "f = open('한국어불용어100.txt', encoding=\"utf8\")\n",
        "for line in f:\n",
        "    word_vector = line.split()\n",
        "    stop_words.append(word_vector[0])    \n",
        "f.close()\n",
        "\n",
        "for i,item in enumerate(splited_test):\n",
        "    for j,k in enumerate(item):\n",
        "        splited_test[i][j]=[word for word in k if not word in stop_words]        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkcvNPIab_Pu"
      },
      "source": [
        "import numpy as np\n",
        "embedding_dict = dict()\n",
        "f = open('glove.txt', encoding=\"utf8\")\n",
        "\n",
        "for line in f:\n",
        "    word_vector = line.split()\n",
        "    word = word_vector[0]\n",
        "    word_vector_arr = np.asarray(word_vector[1:], dtype='float32') # 100개의 값을 가지는 array로 변환\n",
        "    embedding_dict[word] = word_vector_arr\n",
        "f.close()\n",
        "\n",
        "\"\"\"\n",
        "for i in range(len(test_article)):\n",
        "    for j,item in enumerate(test_article[i]):\n",
        "        vector=0\n",
        "        for k in item:\n",
        "            if k in embedding_dict:\n",
        "                vector+=embedding_dict[k]\n",
        "            else:\n",
        "                vector+=np.zeros(100)                \n",
        "        if(len(item)==0):\n",
        "            test_article[i][j]=np.zeros(100) \n",
        "        else:\n",
        "            test_article[i][j]=vector/len(item)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Babc6N8WgGeR"
      },
      "source": [
        "embedding_dim = 100\n",
        "zero_vector = np.zeros(embedding_dim)\n",
        "\n",
        "def calculate_sentence_vector(sentence):\n",
        "  if len(sentence) != 0:\n",
        "    return sum([embedding_dict.get(word, zero_vector) \n",
        "                  for word in sentence])/len(sentence)\n",
        "  else:\n",
        "    return zero_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agfZg_AhnO-8"
      },
      "source": [
        "for i,item in enumerate(splited_test):\n",
        "    for j,k in enumerate(item):\n",
        "        splited_test[i][j]=calculate_sentence_vector(k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCVYkP3Yqi3A"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def similarity_matrix(sentence_embedding):\n",
        "  sim_mat = np.zeros([len(sentence_embedding), len(sentence_embedding)])\n",
        "  for i in range(len(sentence_embedding)):\n",
        "      for j in range(len(sentence_embedding)):\n",
        "        sim_mat[i][j] = cosine_similarity(sentence_embedding[i].reshape(1, 100),\n",
        "                                          sentence_embedding[j].reshape(1, 100))[0,0]\n",
        "  return sim_mat\n",
        "\n",
        "def calculate_score(sim_matrix):\n",
        "    nx_graph = nx.from_numpy_array(sim_matrix)\n",
        "    scores = nx.pagerank(nx_graph)\n",
        "    return scores\n",
        "\n",
        "test = []\n",
        "for line in open('extractive_test_v2.jsonl', 'r'):\n",
        "    test.append(json.loads(line))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GsMlP3GzKDi"
      },
      "source": [
        "submission=[]\n",
        "for i,item in enumerate(test_article):\n",
        "    print(i)\n",
        "    try:\n",
        "        sub=[]\n",
        "        k=similarity_matrix(item)\n",
        "        k=calculate_score(k)\n",
        "        k=sorted(k.items(), reverse=True, key=lambda item:item[1])\n",
        "        for index,(m,n) in enumerate(k):\n",
        "            sub.append(test[i]['article_original'][m])\n",
        "            if(index==2):\n",
        "                break\n",
        "        submission.append(sub)            \n",
        "    \n",
        "    except:\n",
        "        for t in range(3):\n",
        "            sub.append(test[t]['article_original'][t])\n",
        "        submission.append(sub)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}